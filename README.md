# Text-WaterMark-PaperList

Text watermark is an important research direction for the misuse of LLM-generated text.
This repo will have the followings:

- [Text-WaterMark-PaperList](#text-watermark-paperlist)
- [1.BackGround](#1background)
  - [1.1 Application](#11-application)
  - [1.2 Classification](#12-classification)
    - [1.2.1 Bit Count](#121-bit-count)
    - [1.2.2 Embedding Phase](#122-embedding-phase)
    - [1.2.3 Generating-Process](#123-generating-process)
    - [1.2.4 Token-Level](#124-token-level)
    - [1.2.5 Different Scenarios](#125-different-scenarios)
  - [1.3 Necessary Features](#13-necessary-features)
  - [1.4 Attack Function](#14-attack-function)
    - [1.4.1 Word Level](#141-word-level)
    - [1.4.2 Sentence Level](#142-sentence-level)
    - [1.4.3 Spoof Attack](#143-spoof-attack)
- [2.Watermark Paper](#2watermark-paper)
- [3.Survey And Benchmark](#3survey-and-benchmark)
- [4.Attack](#4attack)
- [5.Other Research](#5other-research)
- [Reference](#reference)

# 1.BackGround

## 1.1 Application

- Deep Fake Detection: Detect whether the target text is generated by a language model;
- Deep Fake Attribution: Determine which model/user generated the target text;
- IP Protection: Protect valuable text and model

## 1.2 Classification

- [Bit Count](#121-bit-count)
  - Zero-Bit
  - Multi-Bit
- [Embedding Phase](#122-embedding-phase)
  - Backdoor/A-Priori
  - [Generating-Process](#123-generating-embedding-phase)
    - [Token Level](#124-token-level)
      - Logits Bias
      - Reweighting
      - Sampling
    - Sentence Level
  - Post-Hoc
- [Different-Scenarios](#125-different-scenarios)
  - Competion
  - Condition
  - Code

### 1.2.1 Bit Count

Distinguished based on the amount of information that can be embedded in the watermark.

- **Zero-Bit**: Only determine whether to add watermark, no additional information. This makes the watermark only detect, no attribution effect.
- **Multi-Bit**: Embed and extract multi-bit information, you can freely add the content you want to embed, such as time, user ID, etc. Multi-Bit watermark is necessary for deep fake attribution.

### 1.2.2 Embedding Phase

Text watermark research existed in the last century, at which time it was only possible to add watermarks to already written text, and was classified as Post-hoc. However, with the development of language models, some new stages of text watermark emerges.

- **Backdoor/A-Priori**: By employing specific methods to utilize watermark information to form a toxic dataset, fine-tuning or training a language model can result in *a language model with watermark*. The introduction of watermark information occurs *before the language model generates text*.
- **Generating-Process**: The introduction of watermark information will interfere with the generation process of the language model, resulting in the language model producing text with embedded watermarks. The addition of such watermarks does not require modification of the language model, but it necessitates access to the complete language model.
- **Post-Hoc**: By altering the text to introduce watermark information, the watermarked text is constructed. This kind of method does not require the original generative language model; it only requires the text generated by the model.

### 1.2.3 Generating-Process

This category represents a further subdivision within Category [Embedding Phase→Generating-Process](#122-embedding-phase).

- **Token-Level**: Watermark information is introduced when the language model generates a token each time.
- **Sentence-Level**: The embedding of watermark information is used when the language model generates each sentence. This requires a semi-controlled sampling process when generating: 1.No constraint is imposed until a sentence is generated; 2.Multiple candidates must be obtained using beam search during generation; 3.When generated, it must be generated sentence by sentence.

### 1.2.4 Token-Level

This category represents a further subdivision within Category [Embedding Phase→Generating-Process→Token-Level](#123-generating-process).

During each step of token generation, the language model undergoes the following processing:
Get **Logits** on the vocabulary $\rightarrow$ Get Probability Distribution(**Weight**) over the vocabulary $\rightarrow$ **Sample** from the probability distribution

- **Logits Bias**: The watermark information is converted into a logits distribution on the vocabulary, which is added to the logits distribution generated by the original language model to interfere with the generation phase.
- **Reweighting**: The watermark information will guide reweighting probability distribution. This typically involves scrambling the vocabulary and selecting a re-weighting interval.
- **Sampling**: Watermark information is embedded by influencing the generation results through sampling, which prohibits the commonly used decoding methods and restricts the decoding approach.

### 1.2.5 Different Scenarios

- **Completion**: Normal Generation. Given a prompt and complete it. Most watermarks are configured for this type of scenario.
- **Condition**: Conditional text generation. Such tasks are typically in the form of question-and-answer (QA) and summarization tasks.
- **Code**: Code generation. Generating executable code.

## 1.3 Necessary Features

The addition of watermarks often involves a trade-off among multiple aspects.

- **Detectability**: Detectability refers to the capability of a watermarking method to distinguish between watermarked text and non-watermarked text.
  Metric: Typically, it is a binary classification metric, and in the case of multiple bits, there are corresponding multi-bit comparison metrics.
  - TPR/TNR/FPR/FNR
  - AUROC
  - Accuracy
  - TPR@FPR=X%
  - Bit Accuracy/Bit Error Rate(Multi-Bit)
- **Invisibility/Text Quality**: Invisibility refers to the impact of the watermark on the quality of the generated text.
  Metric: Typically, the text quality metrics of the original generated text and the watermarked text are compared.
  - Perplexity(PPL)
  - BLEU
  - ROUGE(-n)
  - BERTScore
  - Entailment Score(ES)
  - Sentence Similarity
  - Human Evaluate
  - Ent-3[^Ent3]
  - Rep-3[^Rep3]
- **Imperceptibility**: Imperceptibility indicates the impact of watermark addition on the overall token distribution, focusing more on the statistical differences between watermarked and non-watermarked texts, which requires a large amount of corresponding text for overall estimation.
  Metric: Imperceptibility-related metrics often involve token-level statistics for the overall generated text. It assesses whether there are differences in the token distribution between watermarked texts and non-watermarked texts.
  - Word Frequency
- **Robustness**: Robustness refers to the ability of watermarked text to still be recognized as watermarked after undergoing watermark removal attacks.
  Metric: Robustness-related metrics are represented by comparing the changes in detectability indicators caused by attacks.
- **Usability**: Usability refers to the additional time and memory consumption caused by the addition of watermarks. These costs must be within an acceptable range for the user.
  Metric: Usability metrics typically consist of corresponding values for watermarking time and memory usage.
  - Time Cost
  - Memory Cost

## 1.4 Attack Function

### 1.4.1 Word Level

The watermark is removed by modifying the word.

+ **Insert**: Insert words randomly according to proportion.
+ **Delete**: Delete words randomly according to proportion.
+ **Exchange**: Swap the positions of two words in a sentence.
+ **Replace**: Similar substitutions are usually chosen based on the semantics of a particular word.
  + **Synonym**: Choose word substitutes according to the calculated thesaurus or the trained word vector.
  + **Context**: According to the MLM task of Masked Language Model, candidate words are determined by context.
+ **Emoji**: The model is induced to insert emojis according to certain rules.
+ **Exception Word Removal**: ONION, In the inference stage, GPT-2 pre-trained model is used to prevent the activation of backdoors by detecting and removing abnormal words such as "cf" in test samples.
+ **HELM Perturbation**: Deliberately use word variations to remove watermarks.
  + **Structure**: do not --> don't
  + **Case**: Large --> large
  + **Spell/Ghost-Word**: sun --> sunn; sun --> san

### 1.4.2 Sentence Level

The watermark is removed by modifying the sentence.

+ **Human-Modify**: Humans manually modify it directly.
+ **Paraphrase**: Use language models to rewrite sentences. Model: DIPPER;pegasus_paraphrase...
  + Bigram-Paraphrase[^BiParaphrase]: When overwriting, several candidates are generated and the candidate with the least overlap of tokens is selected.
+ **Re-Translate**: Translate into another language, translate back.
+ **Re-Watermark**: Re-add watermark when overwriting.

### 1.4.3 Spoof Attack

The method of cracking the watermark is to try to forge the watermark text in a certain way.

# 2.Watermark Paper

About Key Word:

* The keywords we set are used to describe the most prominent features of the corresponding paper, the absence of some keywords does not mean that the paper does not cover those aspects.
  For example:Zero-Bit, Completion, Detectability, Logits Bias, Token Level.
* Some watermark already have classic abbreviations, which we have also included in the keywords.

|                                                                                                     Paper                                                                                                     | Proceedings / Journal-Year | Key Word                                                                         | Note                                                                                                                                                                                                                               | Code                                                                                                                |
| :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | -------------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- |
|                                                 [A watermark for large language models](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf)                                                 | ICML-2023                  | Zero-Bit,Logits Bias,KGW                                                         | The classical method uses watermark information<br /> to divide the vocabulary into red and green list, <br />and increases the logits of the green list.                                                                          | [https://github.com/jwkirchenbauer/lm-watermarking](https://github.com/jwkirchenbauer/lm-watermarking)                 |
|                                 [Protecting Intellectual Property of Language Generation APIs with Lexical Watermark.](https://ojs.aaai.org/index.php/AAAI/article/view/21321)                                 | AAAI-2022                  | Post-Hoc, Lexical                                                                | Lexical substitute                                                                                                                                                                                                                | [https://github.com/xlhex/NLG_api_watermark.git](https://github.com/xlhex/NLG_api_watermark.git)                       |
| [CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks.](https://proceedings.neurips.cc/paper_files/paper/2022/file/2433fec2144ccf5fea1c9c5ebdbc3924-Paper-Conference.pdf) | NeurIPS-2022               | Post-Hoc,Condition                                                               | Imporve-[Protecting Intellectual Property of <br />Language Generation APIs with Lexical Watermark.](https://ojs.aaai.org/index.php/AAAI/article/view/21321)                                                                          | [https://github.com/xlhex/cater_neurips](https://github.com/xlhex/cater_neurips)                                       |
|                           [Adversarial Watermarking Transformer: Towards Tracing Text Provenancewith Data Hiding.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9519400)                           | SP-2021                    | AWT,Multi-Bit,Post-Hoc                                                           | Train watermark encoder and decoder, embed and<br /> extract watermark to exist text.                                                                                                                                              | [https://github.com/salesforce/awd-lstm-lm](https://github.com/salesforce/awd-lstm-lm)                                 |
|                                            [Protecting Language Generation Models via Invisible Watermarking](https://proceedings.mlr.press/v202/zhao23i/zhao23i.pdf)                                            | ICML-2023                  | Reweighting                                                                      | -                                                                                                                                                                                                                                  | [https://github.com/XuandongZhao/Ginsew](https://github.com/XuandongZhao/Ginsew)                                       |
|                                                          [Watermarking Pre-trained Language Models with Backdooring](https://arxiv.org/pdf/2210.07543)                                                          | 2022                       | Backdoor                                                                         | -                                                                                                                                                                                                                                  | -                                                                                                                   |
|                  [DeepTextMark Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10471537)                  | IEEE-2024                  | Post-Hoc                                                                         | Word substitute and candidate sentence selection                                                                                                                                                                                  | -                                                                                                                   |
|                                             [Tracing Text Provenance via Context-Aware Lexical Substitution](https://ojs.aaai.org/index.php/AAAI/article/view/21415)                                             | AAAI-2022                  | Post-Hoc,Multi-Bit                                                               | Choose candidate word positon and use Bert to<br /> choose words.                                                                                                                                                                  | -                                                                                                                   |
|                                                            [Towards Tracing Code Provenance with Code Watermarking](https://arxiv.org/pdf/2305.12461)                                                            | 2023                       | Code, Post-Hoc                                                                   | -                                                                                                                                                                                                                                  | -                                                                                                                   |
|                                                           [Watermarking Text Generated by Black-Box Language Models](https://arxiv.org/pdf/2305.08883)                                                           | 2023                       | Post-Hoc                                                                         | MLM generates candidate words<br /> and scores candidate sentences                                                                                                                                                                 | [https://github.com/Kiode/Text_Watermark](https://github.com/Kiode/Text_Watermark)                                     |
|                                                            [Who Wrote this Code? Watermarking for Code Generation](https://arxiv.org/pdf/2305.15060)                                                            | 2024                       | SWEET, Logits Bias,<br /> Code, Invisibility                                     | Use entropy threshold to control logits bias.<br />Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf)                                                                                              | [https://github.com/hongcheki/sweet-watermark](https://github.com/hongcheki/sweet-watermark)                           |
|                                 [Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark](https://arxiv.org/pdf/2305.10036)                                 | ACL-2023                   | Backdoor                                                                         | -                                                                                                                                                                                                                                  | [https://github.com/yjw1029/EmbMarker](https://github.com/yjw1029/EmbMarker)                                           |
|                                           [Robust Multi-bit Natural Language Watermarking through Invariant Features](https://aclanthology.org/2023.acl-long.117.pdf)                                           | ACL-2023                   | Post-Hoc,Multi-Bit                                                               | MLM generate candidate words, Select the<br /> candidate at the corresponding position<br /> to correspond to the watermark information                                                                                           | [https://github.com/bangawayoo/nlp-watermarking](https://github.com/bangawayoo/nlp-watermarking)                       |
|                                                                 [Undetectable Watermarks for Language Models](https://arxiv.org/pdf/2306.09194)                                                                 | IACR-2023                  | Sample                                                                           | The embedding process of watermark<br /> is controlled by entropy                                                                                                                                                                  | -                                                                                                                   |
|                                                            [Robust Distortion-free Watermarks for Language Models](https://arxiv.org/pdf/2307.15593)                                                            | 2023                       | Sample,EXP, EXP-Edit                                                             | The sampling related parameters are<br /> generated by the secret key sequence<br /> to interfere with the sampling                                                                                                                | [https://github.com/jthickstun/watermark](https://github.com/jthickstun/watermark)                                     |
|                                       [SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation](https://aclanthology.org/2024.naacl-long.226.pdf)                                       | NAACL-2024                 | Sentence-Level,<br />Semstamp                                                    | Sentence level red/green list. Imporve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),                                                                                                                 | [https://github.com/bohanhou14/SemStamp](https://github.com/bohanhou14/SemStamp)                                       |
|                                   [Three Bricks to Consolidate Watermarks for Large Language Models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10374576&tag=1)                                   | WIFS-2023                  | Logits-Bias, Detectability                                                       | Imporve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf), Change the detection method to <br />improve the relative detection accuracy                                                                  | -                                                                                                                   |
|                                               [PromptCARE: Prompt Copyright Protection by Watermark Injection and Verification](https://arxiv.org/pdf/2308.02816)                                               | 2023                       | Backdoor,PromptCARE                                                              | For prompt as a service                                                                                                                                                                                                            | -                                                                                                                   |
|                                                                    [Embarrassingly Simple Text Watermarks](https://arxiv.org/pdf/2310.08920)                                                                    | 2023                       | Post-Hoc                                                                         | Add watermark with invisible/similar Unicode<br /> characters                                                                                                                                                                      | [Not Code][https://easymarkdemo.github.io/](https://easymarkdemo.github.io/)                                           |
|                                                             [Publicly Detectable Watermarking for Language Models](https://arxiv.org/pdf/2310.18491)                                                             | IACR-2023                  | Sample                                                                           | A relatively complicated method                                                                                                                                                                                                    | [https://github.com/jfairoze/publicly-detectable-watermark](https://github.com/jfairoze/publicly-detectable-watermark) |
|                                      [Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring](https://arxiv.org/pdf/2311.09668)                                      | 2023                       | Logits-Bias, Invisibility                                                       | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),Generate a score for each <br />step to determine whether to add a watermark                                                                    | -                                                                                                                   |
|                                                       [WatME: Towards Lossless Watermarking Through Lexical Redundancy](https://arxiv.org/pdf/2311.09832)                                                       | 2024                       | Logits-Bias, Invisibility                                                        | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),Use synonyms to interfere with <br />the red-green list division                                                                                | [https://github.com/ChanLiang/WatME](https://github.com/ChanLiang/WatME)                                               |
|                                                        [Necessary and sufficient watermark for large language models.](https://arxiv.org/pdf/2310.00833)                                                        | 2023                       | Sample, Invisibility                                                             | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),By controlling the proportion <br />of green tokens, the text quality can be<br /> improved while the detection performance<br /> is guaranteed | -                                                                                                                   |
|                                                             [Provable robust watermarking for ai-generated text.](https://arxiv.org/pdf/2306.17439)                                                             | 2023                       | Logits-Bias, Invisibility,<br />Robustness,Detectability,<br />Usability,Unigram | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),Fixed red-green list for <br />better performance                                                                                               | [https://github.com/XuandongZhao/Unigram-Watermark](https://github.com/XuandongZhao/Unigram-Watermark)                 |
|                 [Watermarking conditional text generation for ai detection: Unveiling challenges and a semantic-aware watermark remedy.](https://ojs.aaai.org/index.php/AAAI/article/view/29756)                 | AAAI-2024                  | Logits-Bias, Invisibility,<br />Condition                                        | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),Use semantics to divide <br />red and green lists to improve text quality                                                                       | -                                                                                                                   |
|                                                       [A Semantic Invariant Robust Watermark for Large Language Models](https://arxiv.org/pdf/2310.06356)                                                       | ICLR-2024                  | Logits-Bias,Robustness,<br />Invisibility,Imperceptibility,<br />SIR             | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),Generate a logits bias <br />based on semantics to add a watermark                                                                              | [https://github.com/THU-BPM/Robust_Watermark](https://github.com/THU-BPM/Robust_Watermark)                             |
|                                                                   [Towards Optimal Statistical Watermarking](https://arxiv.org/pdf/2312.07930)                                                                   | 2024                       | Generating-Process,UMP                                                           | The statistical value of watermark judgment<br />is controlled to achieve the best effect                                                                                                                                          | -                                                                                                                   |
|                                        [REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models](https://arxiv.org/pdf/2310.12362)                                        | USENIX-2023                | Sample, Logits-Bias,<br />Multi-Bit                                              | -                                                                                                                                                                                                                                  | -                                                                                                                   |
|                                            [Can Watermarks Survive Translation?On the Cross-lingual Consistency of Text Watermark](https://arxiv.org/pdf/2402.14007)                                            | 2024                       | Logits-Bias, Detectability                                                       | This paper aims at the problem that the<br />watermark strength decreases when the <br />text in A language is translated into B <br />language. Improve-[SIR](https://arxiv.org/pdf/2310.06356)                                      | [https://github.com/zwhe99/X-SI](https://github.com/zwhe99/X-SIR)                                                      |
|                                          [k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text](https://arxiv.org/pdf/2402.11399)                                          | 2024                       | Sentence-Level,<br />k-SemStamp                                                  | Imporve-[SemStamp](https://aclanthology.org/2024.naacl-long.226.pdf),                                                                                                                                                                 | [https://github.com/bohanhou14/SemStamp](https://github.com/bohanhou14/SemStamp)                                       |
|                                   [Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models](https://arxiv.org/pdf/2402.18059)                                   | 2024                       | Logits-Bias                                                                      | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),Dynamically adjust the <br />hyperparameters at generation time                                                                                 | [https://github.com/mignonjia/TS_watermark](https://github.com/mignonjia/TS_watermark)                                 |
|                                                    [WaterMax: breaking the LLM watermark detectability-robustness-quality](https://arxiv.org/pdf/2403.04808)                                                    | 2024                       | Sample                                                                           | Select text with a high watermark score<br />using beam search                                                                                                                                                                     | -                                                                                                                   |
|                                                              [Adaptive Text Watermark for Large Language Models](https://arxiv.org/pdf/2401.13927)                                                              | ICML-2024                  | Logits-Bias, Invisibility                                                        | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),Similar to [SIR](https://arxiv.org/pdf/2310.06356).Use the model <br />to determine logits bias                                                    | [https://github.com/yepengliu/adaptive-text-watermark](https://github.com/yepengliu/adaptive-text-watermark)           |
|                                            [Cross-Attention Watermarking of Large Language Models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10446397)                                            | ICASSP-2024                | Post-Hoc,Multi-Bit                                                               | Imporve-[AWT](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9519400)                                                                                                                                                       | [https://gitlab.com/folbaeni/linguistic-watermark](https://gitlab.com/folbaeni/linguistic-watermark)                   |
|                                                       [Multi-Bit Distortion-Free Watermarking for Large Language Models](https://arxiv.org/pdf/2402.16578)                                                       | 2024                       | Sample,Multi-Bit                                                                 | Imporve-[Undetectable Watermarks <br />for Language Models](https://arxiv.org/pdf/2306.09194)                                                                                                                                         | -                                                                                                                   |
|                                                [An Unforgeable Publicly Verifiable Watermark for Large Language Models](https://openreview.net/pdf?id=gMLQwKDY3N)                                                | ICLR-2024                  | Logits-Bias,UPV                                                                  | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf)                                                                                                                                                 | [https://github.com/THU-BPM/unforgeable_watermark](https://github.com/THU-BPM/unforgeable_watermark)                   |
|                                            [A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models](https://arxiv.org/pdf/2310.07710)                                            | 2024                       | Logits-Bias,DiPmark                                                              | Imporve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf)                                                                                                                                                 | [https://github.com/yihwu/DiPmark](https://github.com/yihwu/DiPmark)                                                   |
|                                                   [Towards Codable Watermarking for Injecting Multi-bit Information to LLM](https://arxiv.org/pdf/2307.15992)                                                   | 2024                       | Multi-Bit,Logits-Bias                                                            | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf)                                                                                                                                                 | [https://github.com/lancopku/codable-watermarking-for-llm](https://github.com/lancopku/codable-watermarking-for-llm)   |
|                                                                [Resilient Watermarking for LLM-Generated Codes](https://arxiv.org/pdf/2402.07518)                                                                | 2024                       | Post-Hoc,Code,ACW                                                                | -                                                                                                                                                                                                                                  | [https://github.com/boutiquelee/ACW](https://github.com/boutiquelee/ACW)                                               |
|                                                   [Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs](https://arxiv.org/pdf/2402.05864)                                                   | 2024                       | Sample                                                                           | -                                                                                                                                                                                                                                  | [https://github.com/XuandongZhao/pf-decoding](https://github.com/XuandongZhao/pf-decoding)                             |
|                      Not Paper:[https://simons.berkeley.edu/talks/scott-aaronson-ut-austin-openai-2023-08-17](https://simons.berkeley.edu/talks/scott-aaronson-ut-austin-openai-2023-08-17)                      | 2023                       | AAR                                                                              | -                                                                                                                                                                                                                                  | -                                                                                                                   |
|                                                             [An Entropy-based Text Watermarking Detection Method](https://arxiv.org/pdf/2403.13485)                                                             | ACL-2024                   | Sample,Condition                                                                 | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf), The score of detection <br />is determined dynamically according <br />to entropy                                                              | [https://github.com/luyijian3/EWD](https://github.com/luyijian3/EWD)                                                   |
|                                                               [Duwak: Dual Watermarks in Large Language Models](https://arxiv.org/pdf/2403.13000)                                                               | 2024                       | Sample,Logits-Bias                                                               | double watermark,Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),Improve-[EXP](https://arxiv.org/pdf/2307.15593),                                                                                  | -                                                                                                                   |
|                                                              [Is Watermarking LLM-Generated Code Robust?](https://openreview.net/pdf?id=8PhI1PzSYY)                                                              | ICLR-2024                  | Code,Post-Hoc                                                                    | -                                                                                                                                                                                                                                  | [https://github.com/uiuc-arc/llm-code-watermar](https://github.com/uiuc-arc/llm-code-watermark)                        |
|                                                     [Learning to Watermark LLM-generated Text via Reinforcement Learning](https://arxiv.org/pdf/2403.10553)                                                     | 2024                       | Backdoor                                                                         | Use reinforcement learning to quickly embed<br />backdoors                                                                                                                                                                         | [https://github.com/xiaojunxu/learning-to-watermark-llm](https://github.com/xiaojunxu/learning-to-watermark-llm)       |
|                                                      [Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning](https://arxiv.org/pdf/2402.14883)                                                      | 2024                       | Backdoor,                                                                        | -                                                                                                                                                                                                                                  | -                                                                                                                   |
|                                                               [Stylometric Watermarks for Large Language Models](https://arxiv.org/pdf/2405.08400)                                                               | 2024                       | Sample,Logits-Bias                                                               | -                                                                                                                                                                                                                                  | -                                                                                                                   |

# 3.Survey And Benchmark

| Paper                                                                                              | Proceedings / Journal-Year | Type      |
| -------------------------------------------------------------------------------------------------- | -------------------------- | --------- |
| [A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions]()               | 2024                       | Survey    |
| [A Survey of Text Watermarking in the Era of Large Language Models](https://arxiv.org/pdf/2312.07913) | 2024                       | Survey    |
| [Mark My Words: Analyzing and Evaluating Language Model Watermarks](https://arxiv.org/pdf/2312.00273) | 2023                       | Benchmark |
| [Copyright Protection in Generative AI: A Technical Perspective](https://arxiv.org/pdf/2402.02333)    | 2024                       | Survey    |

# 4.Attack

In this part, the core of the paper is the research of the watermarking attack method, and sometimes the corresponding solution is proposed.

|                                                           Paper                                                           | Proceedings / Journal-Year | AttackType            | Note                                                                                                                                         | Code                                                                                                                              |
| :-----------------------------------------------------------------------------------------------------------------------: | -------------------------- | --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
|                  [On the Learnability of Watermarks for Language Models](https://arxiv.org/pdf/2312.04469)                  | ICLR-2024                  | Spoof                 | Two different distillation methods to<br />learn watermarking information                                                                    | [https://github.com/chenchenygu/watermark-learnability](https://github.com/chenchenygu/watermark-learnability)                       |
|    [Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models](https://arxiv.org/pdf/2311.04378)    | 2024                       | Paraphrase,Word Level | A practical method to completely remove<br />the watermark, but the complexity is relatively high                                            | [https://github.com/hlzhang109/impossibility-watermark](https://github.com/hlzhang109/impossibility-watermark)                       |
|                       [Watermark Stealing in Large Language Models](https://arxiv.org/pdf/2402.19361)                       | ICML-2024                  | Spoof                 | For-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),Using n-gram score interference <br />to imitate watermarking | [https://github.com/eth-sri/watermark-stealing](https://github.com/eth-sri/watermark-stealing)                                       |
| [Paraphrasing evades detectors of AI-generated text,but retrieval is an effective defense](https://arxiv.org/pdf/2303.13408) | NeurIPS-2024               | Paraphrase,DIPPER     | -                                                                                                                                            | [https://github.com/martiansideofthemoon/ai-detection-paraphrases](https://github.com/martiansideofthemoon/ai-detection-paraphrases) |
|                       [Can AI-Generated Text be Reliably Detected?](https://arxiv.org/pdf/2303.11156)                       | 2024                       | Spoof,Paraphrase      | The result of multiple detection methods under<br />different attacks                                                                        | [https://github.com/vinusankars/Reliability-of-AI-text-detectors](https://github.com/vinusankars/Reliability-of-AI-text-detectors)   |
|                  [Lost in Overlap: Exploring Watermark Collision in LLMs](https://arxiv.org/pdf/2403.10020)                  | 2024                       | Paraphrase            | -                                                                                                                                            | -                                                                                                                                 |
|                                                                                                                          |                            |                       |                                                                                                                                              |                                                                                                                                   |
|                                                                                                                          |                            |                       |                                                                                                                                              |                                                                                                                                   |
|                                                                                                                          |                            |                       |                                                                                                                                              |                                                                                                                                   |

# 5.Other Research

|                                                                      Paper                                                                      | Proceedings / Journal-Year | Detail                                                                                                                                                                        | Code                                                                                      |
| :---------------------------------------------------------------------------------------------------------------------------------------------: | -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
|                          [Performance Trade-offs of Watermarking Large Language Models](https://arxiv.org/pdf/2311.09816)                          | 2023                       | Performance variation of [[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf)] on different NLP tasks                                                 | -                                                                                         |
|               [New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking](https://openreview.net/pdf?id=PuhF0hyDq1)               | TMLR-2023                  | Evaluation Method                                                                                                                                                             | [https://github.com/su-karanps/watermark_eval](https://github.com/su-karanps/watermark_eval) |
|                                [Optimizing watermarks for large language models](https://arxiv.org/pdf/2312.17295)                                | 2023                       | Improve-[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf), Converting watermarking to optimization problem.                                         | -                                                                                         |
|                                 [Watermarking Makes Language Models Radioactive](https://arxiv.org/pdf/2402.14904)                                 | 2024                       | Radioactive, Learnability about watermark.The main concern is<br />whether the watermarked model output can affect the training <br />model when it is used as training data. | -                                                                                         |
|                  [No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices](https://arxiv.org/pdf/2402.16187)                  | 2024                       | Test about some watermarking attack methods.                                                                                                                                  | -                                                                                         |
|                          [Baselines for Identifying Watermarked Large Language Models](https://arxiv.org/pdf/2305.18456)                          | 2023                       | Identify whether the language model contains watermarks<br />based on the output                                                                                              | -                                                                                         |
|                [WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models](https://arxiv.org/pdf/2403.19548)                | NAACL                      | A large number of experiments for[KGW](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf),and a new evaluation<br /> method                                 | -                                                                                         |
| [A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules](https://arxiv.org/pdf/2404.01245) | 2024                       | Watermark hyperparameter optimization framework                                                                                                                               | -                                                                                         |
|                              [MARKLLM: An Open-Source Toolkit for LLM Watermarking](https://arxiv.org/pdf/2405.10051)                              | 2024                       | Watermarking integration tool                                                                                                                                                 |                                                                                           |

# Reference

[^Ent3]: [Generating informative and diverse conversational responses via adversarial information maximization.](https://proceedings.neurips.cc/paper_files/paper/2018/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf)
    
[^Rep3]: [Neural text generation with unlikelihood training](https://openreview.net/forum?id=SJeYe0NtvH)
    
[^BiParaphrase]: [SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation](https://aclanthology.org/2024.naacl-long.226.pdf)
